{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8f3c2f-3211-4f0f-9cf9-f68b95c66e3e",
   "metadata": {},
   "source": [
    "# ECG time series clustering using Self organising maps and Dynamic Time Warping Barycenter Averaging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5f6ef95-1601-4f04-b554-f1359d0d86dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Native libraries\n",
    "import os\n",
    "import math\n",
    "# Essential Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Algorithms\n",
    "from minisom import MiniSom\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bea6e2-8f03-4547-8590-fa170badf9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_som_series_dba_center(som_x, som_y, win_map):\n",
    "    fig, axs = plt.subplots(som_x,som_y,figsize=(5,5))\n",
    "    fig.suptitle('Clusters')\n",
    "    for x in range(som_x):\n",
    "        for y in range(som_y):\n",
    "            cluster = (x,y)\n",
    "            if cluster in win_map.keys():\n",
    "                for series in win_map[cluster]:\n",
    "                    print(series)\n",
    "                    axs[cluster].plot(series,c=\"gray\",alpha=0.5) \n",
    "                axs[cluster].plot(dtw_barycenter_averaging(np.vstack(win_map[cluster])),c=\"red\") # I changed this part\n",
    "            cluster_number = x*som_y+y+1\n",
    "            axs[cluster].set_title(f\"Cluster {cluster_number}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed73ba7-b49a-4d12-a53f-15976ad1eadf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Little handy function to plot series\n",
    "def plot_som_series_averaged_center(som_x, som_y, win_map):\n",
    "    fig, axs = plt.subplots(som_x,som_y,figsize=(25,25))\n",
    "    fig.suptitle('Clusters')\n",
    "    for x in range(som_x):\n",
    "        for y in range(som_y):\n",
    "            cluster = (x,y)\n",
    "            if cluster in win_map.keys():\n",
    "                for series in win_map[cluster]:\n",
    "                    print(series)\n",
    "                    axs[cluster].plot(series,c=\"gray\",alpha=0.5) \n",
    "                axs[cluster].plot(np.average(np.vstack(win_map[cluster]),axis=0),c=\"red\")\n",
    "            cluster_number = x*som_y+y+1\n",
    "            axs[cluster].set_title(f\"Cluster {cluster_number}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5b940-5736-4665-8de5-6115368408c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(heartbeats))))\n",
    "# I didn't see its significance but to make the map square,\n",
    "# I calculated square root of map size which is \n",
    "# the square root of the number of series\n",
    "# for the row and column counts of som\n",
    "\n",
    "som = MiniSom(10, 10, len(heartbeats[0]), sigma=0.3, learning_rate = 0.1)\n",
    "\n",
    "som.random_weights_init(heartbeats)\n",
    "som.train(heartbeats, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b2f0b-628b-4f26-aad7-3c773de81005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "win_map = som.win_map(heartbeats)\n",
    "# Returns the mapping of the winner nodes and inputs\n",
    "\n",
    "plot_som_series_averaged_center(som_x, som_y, win_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba9521-aaaf-42f4-8e62-50a9a47af3f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "win_map = som.win_map(heartbeats)\n",
    "\n",
    "plot_som_series_dba_center(som_x, som_y, win_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb6e14-dfe6-4680-ae5f-adf47bc2d94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fad10c-f00c-4943-bb88-6960e54ad0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9e46c-43c0-4db9-b05e-c8b1832f1dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff129dce-cb32-4841-be1f-742f1e43a645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c298b2-e0b5-4955-ab5a-c01cb599a608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130cd445-dba0-4dc3-b4d3-b791e6c68355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a30204e2-a374-444e-9692-7bd8056dcb4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first identify separat heart beats and use them instead whole ecg signal\n",
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14e57d5a-1b6b-4d81-a30e-e3b0ca10d156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "database_to_frequency = {\n",
    "    'BIDMC': 125,\n",
    "    'Fantasia': 250,\n",
    "    'Capnobase': 300\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f22e1f27-d7fd-43ab-8101-65b5b201fb1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to create epochs\n",
    "def extract_heartbeats(cleaned, peaks, sampling_rate=None): \n",
    "    heartbeats = nk.epochs_create(cleaned, \n",
    "                                  events=peaks, \n",
    "                                  epochs_start=-0.3, \n",
    "                                  epochs_end=0.4, \n",
    "                                  sampling_rate=sampling_rate)\n",
    "    heartbeats = nk.epochs_to_df(heartbeats)\n",
    "    return heartbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7c3b5c6-09d1-4f1d-9efa-16e1664ef782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ecg_to_heartbeats(row):\n",
    "    ecg_signal = row['ECG_Signal']\n",
    "    patient = row['Record']\n",
    "    fs = database_to_frequency[row['Database']]\n",
    "    bidmc_heartbeats = []\n",
    "    try:\n",
    "        signals, info = nk.ecg_process(ecg_signal, sampling_rate=fs)  \n",
    "    except Exception as ex:\n",
    "        signals, info = nk.ecg_process(ecg_signal, sampling_rate=fs, method='pantompkins1985')\n",
    "    \n",
    "    finally:   \n",
    "        rpeaks = info[\"ECG_R_Peaks\"]\n",
    "        cleaned_ecg = signals[\"ECG_Clean\"]\n",
    "\n",
    "        heartbeats = extract_heartbeats(cleaned_ecg, peaks=rpeaks, sampling_rate=125)\n",
    "        heartbeats_pivoted = heartbeats.pivot(index='Time', columns='Label', values='Signal')\n",
    "\n",
    "        labels = list(heartbeats_pivoted)\n",
    "        for label in labels:\n",
    "            bidmc_heartbeats.append(heartbeats_pivoted[label].values.tolist())\n",
    "    return bidmc_heartbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c346788-ba6f-45e8-bf60-da7a5f9b8e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bidmc_df = pd.read_parquet('bidmc_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0586e21-3906-48b1-83aa-e4fcd30f4922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bidmc_df['heartbeats'] = bidmc_df.apply(ecg_to_heartbeats, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0353023f-85e1-4bf1-bf81-25c16515fef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fantasia_df = pd.read_parquet('fantasia_data.parquet')\n",
    "fantasia_df['heartbeats'] = fantasia_df.apply(ecg_to_heartbeats, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce641675-55e1-42c8-9ac0-dc13edd94f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "capnobase_df = pd.read_parquet('capnobase_data.parquet')\n",
    "capnobase_df['heartbeats'] = capnobase_df.apply(ecg_to_heartbeats, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7d056b9-397d-4fef-88d6-3036355b0798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bidmc_df.to_parquet('data/bidmc_heartbeats.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b5a15e91-e55d-42bd-8613-d3d67d2746ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "capnobase_df.to_parquet('data/capnobase_heartbeats.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd0c7ac5-4832-457f-acf3-41a66845c0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fantasia_df.to_parquet('data/fantasia_heartbeats.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d46c43-508c-486b-8fb5-1981aa50229d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.concat([bidmc_df['heartbeats'].explode('heartbeats'), fantasia_df['heartbeats'].explode('heartbeats'), capnobase_df['heartbeats'].explode('heartbeats')]).to_frame('heartbeats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71cf28-814e-4f0b-843b-fe8ae14a4cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c4a82-8229-4220-ba66-67ac2c57a264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_parquet('heartbeats.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad5bba-f8e0-404d-b6e1-574c34592b17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet('heartbeats.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838f035-b6f7-4cdc-88b7-a4fbc9350eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e086f3-29c6-4806-971b-25e09eab8a26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc96de-77f5-4e1c-9356-45058ebf4d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['heartbeats_scaled'] = df['heartbeats'].apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f39e5-9c76-4a2e-a282-6c54509b97b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde0848-12bb-4886-a213-0223ae60fa70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heartbeats = df['heartbeats'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c15c38-33be-4695-8be1-10b9a61d6453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d23bca-0873-403f-a39c-6f56c5256767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File to store pairwise distances incrementally\n",
    "distances_file = \"pairwise_distances.txt\"\n",
    "\n",
    "num_heartbeats = len(heartbeats)\n",
    "dtw_matrix = np.zeros((num_heartbeats, num_heartbeats))\n",
    "\n",
    "# Compute the DTW distances and write to file\n",
    "with open(distances_file, 'w') as f:\n",
    "    for i in range(num_heartbeats):\n",
    "        for j in range(i+1, num_heartbeats):\n",
    "            distance, _ = fastdtw([heartbeats[i]], [heartbeats[j]], dist=euclidean)\n",
    "            # Write the distances in the format: i,j,distance\n",
    "            f.write(f\"{i},{j},{distance}\\n\")\n",
    "\n",
    "# To populate the matrix from the file:\n",
    "dtw_matrix = np.zeros((num_heartbeats, num_heartbeats))\n",
    "with open(distances_file, 'r') as f:\n",
    "    for line in f:\n",
    "        i, j, distance = map(float, line.strip().split(\",\"))\n",
    "        i, j = int(i), int(j)\n",
    "        dtw_matrix[i, j] = distance\n",
    "        dtw_matrix[j, i] = distance  # since the matrix is symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22abf42d-7708-41e7-938b-c0e93f6861b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "# Initialize an empty matrix for DTW distances\n",
    "num_heartbeats = len(heartbeats)\n",
    "dtw_matrix = np.zeros((num_heartbeats, num_heartbeats))\n",
    "\n",
    "# Compute the DTW distances\n",
    "for i in range(num_heartbeats):\n",
    "    for j in range(i+1, num_heartbeats):\n",
    "        distance, _ = fastdtw([heartbeats[i]], [heartbeats[j]], dist=euclidean)\n",
    "        dtw_matrix[i, j] = distance\n",
    "        dtw_matrix[j, i] = distance  # since the matrix is symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3071663f-d4b3-4eca-94de-61ef9d4234b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def compute_dtw(i, heartbeats):\n",
    "    distances = []\n",
    "    for j in range(i+1, len(heartbeats)):\n",
    "        distance, _ = fastdtw([heartbeats[i]], [heartbeats[j]], dist=euclidean)\n",
    "        distances.append((i, j, distance))\n",
    "    return distances\n",
    "\n",
    "num_heartbeats = len(heartbeats)\n",
    "dtw_matrix = np.zeros((num_heartbeats, num_heartbeats))\n",
    "\n",
    "# Compute the DTW distances in parallel using joblib\n",
    "results = Parallel(n_jobs=-1)(delayed(compute_dtw)(i, heartbeats) for i in range(num_heartbeats))\n",
    "\n",
    "# Fill the dtw_matrix with computed distances\n",
    "for distances in results:\n",
    "    for i, j, distance in distances:\n",
    "        dtw_matrix[i, j] = distance\n",
    "        dtw_matrix[j, i] = distance  # since the matrix is symmetric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8ea40-5acc-4514-97bb-635bd6ef3d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File to store pairwise distances incrementally\n",
    "distances_file = \"pairwise_distances.txt\"\n",
    "\n",
    "# Compute the DTW distances and write to file\n",
    "with open(distances_file, 'w') as f:\n",
    "    for i in range(num_heartbeats):\n",
    "        for j in range(i+1, num_heartbeats):\n",
    "            distance, _ = fastdtw(heartbeats[i], heartbeats[j], dist=euclidean)\n",
    "            # Write the distances in the format: i,j,distance\n",
    "            f.write(f\"{i},{j},{distance}\\n\")\n",
    "\n",
    "# To populate the matrix from the file:\n",
    "dtw_matrix = np.zeros((num_heartbeats, num_heartbeats))\n",
    "with open(distances_file, 'r') as f:\n",
    "    for line in f:\n",
    "        i, j, distance = map(float, line.strip().split(\",\"))\n",
    "        i, j = int(i), int(j)\n",
    "        dtw_matrix[i, j] = distance\n",
    "        dtw_matrix[j, i] = distance  # since the matrix is symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7de84b-b45d-4eed-89f1-1756265e567f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Much of the code is modified from:\n",
    "- https://codesachin.wordpress.com/2015/11/28/self-organizing-maps-with-googles-tensorflow/\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    " \n",
    "\n",
    "class SOM(nn.Module):\n",
    "    \"\"\"\n",
    "    2-D Self-Organizing Map with Gaussian Neighbourhood function\n",
    "    and linearly decreasing learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, dim, niter, alpha=None, sigma=None):\n",
    "        super(SOM, self).__init__()\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.dim = dim\n",
    "        self.niter = niter\n",
    "        if alpha is None:\n",
    "            self.alpha = 0.3\n",
    "        else:\n",
    "            self.alpha = float(alpha)\n",
    "        if sigma is None:\n",
    "            self.sigma = max(m, n) / 2.0\n",
    "        else:\n",
    "            self.sigma = float(sigma)\n",
    "\n",
    "        self.weights = torch.randn(m*n, dim)\n",
    "        self.locations = torch.LongTensor(np.array(list(self.neuron_locations())))\n",
    "        self.pdist = nn.PairwiseDistance(p=2)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def get_locations(self):\n",
    "        return self.locations\n",
    "\n",
    "    def neuron_locations(self):\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.n):\n",
    "                yield np.array([i, j])\n",
    "\n",
    "    def map_vects(self, input_vects):\n",
    "        to_return = []\n",
    "        for vect in input_vects:\n",
    "            min_index = min([i for i in range(len(self.weights))],\n",
    "                            key=lambda x: np.linalg.norm(vect-self.weights[x]))\n",
    "            to_return.append(self.locations[min_index])\n",
    "\n",
    "        return to_return\n",
    "\n",
    "    def forward(self, x, it):\n",
    "        dists = self.pdist(torch.stack([x for i in range(self.m*self.n)]), self.weights)\n",
    "        _, bmu_index = torch.min(dists, 0)\n",
    "        bmu_loc = self.locations[bmu_index,:]\n",
    "        bmu_loc = bmu_loc.squeeze()\n",
    "        \n",
    "        learning_rate_op = 1.0 - it/self.niter\n",
    "        alpha_op = self.alpha * learning_rate_op\n",
    "        sigma_op = self.sigma * learning_rate_op\n",
    "\n",
    "        bmu_distance_squares = torch.sum(torch.pow(self.locations.float() - torch.stack([bmu_loc for i in range(self.m*self.n)]).float(), 2), 1)\n",
    "        \n",
    "        neighbourhood_func = torch.exp(torch.neg(torch.div(bmu_distance_squares, sigma_op**2)))\n",
    "        \n",
    "        learning_rate_op = alpha_op * neighbourhood_func\n",
    "\n",
    "        learning_rate_multiplier = torch.stack([learning_rate_op[i:i+1].repeat(self.dim) for i in range(self.m*self.n)])\n",
    "        delta = torch.mul(learning_rate_multiplier, (torch.stack([x for i in range(self.m*self.n)]) - self.weights))                                         \n",
    "        new_weights = torch.add(self.weights, delta)\n",
    "        self.weights = new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82545406-8968-45f7-b1fe-6ce38343f0e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = list()\n",
    "for i in range(len(heartbeats)):\n",
    "    data.append(torch.FloatTensor(heartbeats[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71b4bf-3f5d-4cf2-832f-12bab51b2d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = 10\n",
    "n = 10\n",
    "#Train a 20x30 SOM with 100 iterations\n",
    "n_iter = 100\n",
    "som = SOM(m, n, 88, n_iter)\n",
    "for iter_no in range(n_iter):\n",
    "    #Train with each vector one by one\n",
    "    for i in range(len(data)):\n",
    "        som(data[i], iter_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f19f8-21f3-4403-b7a6-6979e3d5e786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Store a centroid grid for easy retrieval later on\n",
    "centroid_grid = [[] for i in range(m)]\n",
    "weights = som.get_weights()\n",
    "locations = som.get_locations()\n",
    "for i, loc in enumerate(locations):\n",
    "    centroid_grid[loc[0]].append(weights[i].numpy())\n",
    " \n",
    "#Get output grid\n",
    "image_grid = centroid_grid\n",
    "\n",
    "#Map colours to their closest neurons\n",
    "mapped = som.map_vects(torch.Tensor(colors))\n",
    "\n",
    "#Plot\n",
    "plt.imshow(image_grid)\n",
    "plt.title('Color SOM')\n",
    "for i, m in enumerate(mapped):\n",
    "    plt.text(m[1], m[0], color_names[i], ha='center', va='center',\n",
    "             bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python research env",
   "language": "python",
   "name": "research-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
